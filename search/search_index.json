{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Korali Korali is a high performance computing framework for optimization, sampling and Bayesian uncertainty quantification of large scale computational models. Korali is based on the TORC task-parallel library for clusters, which is designed to provide unified programming and runtime support for computing platforms that range from single-core systems to hybrid multicore-GPU clusters and heterogenous grid based supercomputers. What Korali does for you Optimize : given a cost function F(\\vartheta) F(\\vartheta) find $$ \\vartheta^\\star = \\mathop{\\arg\\min}\\limits_{\\vartheta} \\,\\,\\, F(\\vartheta) \\,. $$ Sample : given the density of a probability distribution p_{\\vartheta} p_{\\vartheta} draw samples, $$ \\vartheta^{(k)} \\sim p_\\vartheta, \\quad k=1,\\ldots,N_s \\, . $$ Uncertainty Quantification : given a set of data d d , the output of the model f(x;\\vartheta) f(x;\\vartheta) a likelihood function p(d|\\vartheta) p(d|\\vartheta) and a prior probablity density p(\\vartheta) p(\\vartheta) sample the posterior distribution, $$ p(\\vartheta | d) = \\frac{p(d | \\vartheta) \\, p(\\vartheta)}{p(d)}\\, . $$ The model output f f depends on a set of input parameters x x . After installing the software, please have a look at the examples and learn how to run your own code. Warning The software and the documentation page are under continuous development. New pages and new feature will be constantly added.","title":"Home"},{"location":"#welcome-to-korali","text":"Korali is a high performance computing framework for optimization, sampling and Bayesian uncertainty quantification of large scale computational models. Korali is based on the TORC task-parallel library for clusters, which is designed to provide unified programming and runtime support for computing platforms that range from single-core systems to hybrid multicore-GPU clusters and heterogenous grid based supercomputers.","title":"Welcome to Korali"},{"location":"#what-korali-does-for-you","text":"Optimize : given a cost function F(\\vartheta) F(\\vartheta) find $$ \\vartheta^\\star = \\mathop{\\arg\\min}\\limits_{\\vartheta} \\,\\,\\, F(\\vartheta) \\,. $$ Sample : given the density of a probability distribution p_{\\vartheta} p_{\\vartheta} draw samples, $$ \\vartheta^{(k)} \\sim p_\\vartheta, \\quad k=1,\\ldots,N_s \\, . $$ Uncertainty Quantification : given a set of data d d , the output of the model f(x;\\vartheta) f(x;\\vartheta) a likelihood function p(d|\\vartheta) p(d|\\vartheta) and a prior probablity density p(\\vartheta) p(\\vartheta) sample the posterior distribution, $$ p(\\vartheta | d) = \\frac{p(d | \\vartheta) \\, p(\\vartheta)}{p(d)}\\, . $$ The model output f f depends on a set of input parameters x x . After installing the software, please have a look at the examples and learn how to run your own code. Warning The software and the documentation page are under continuous development. New pages and new feature will be constantly added.","title":"What Korali does for you"},{"location":"hpc/","text":"TODO","title":"Performance"},{"location":"installation/","text":"Installation Prerequisities An MPI implementation must be installed on your system, preferably with full thread safety, e.g., MPICH . GSL-2.4 or later must be installed on your system. Installation steps 0. Download Download the Korali project from GitHub . 1. Torc Library The Torc library is shipped with the Korali code. Torc is tasking library that enables platform-independent code. In this example we assume that the MPI compiler is named mpicc: cd lib/torc_lite autoreconf ./configure CC = mpicc --prefix = $HOME /usr/torc make ; make install export PATH = $HOME /usr/torc/bin: $PATH After installing torc, the following flags are available: torc_cflags torc_libs 2. GSL Library The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. Download and unpack the latest version of GSL (outside of korali). Execute following commands in your terminal: wget http://mirror.easyname.at/gnu/gsl/gsl-latest.tar.gz tar -xvzf gsl-latest.tar.gz Configure GSL. Set the install folder to be /$HOME/usr . If you want to install in the default diretcory, /usr/local , delete the --prefix=/$HOME/usr . ./configure --prefix = / $HOME /usr Compile and install GSL. make -j2 make install This step will take some time. If your machine has more than two cores, set the 2 in the -j2 to that number number. GSL Installation Hints Read the INSTALL notes of GSL. Update the path variable of your terminal with the location of the binaries (gsl-config, gsl-histogram and gsl-randist). Set the LD_LIBRARY_PATH variable of your terminal to the newly installed lib folder. 3. Korali Installation Enter the build directory: cd build Before compiling, the following need to be checked: Name for the MPI compiler in the source/make/common.mk , since this can be named differently on different platforms (e.g. CC=cc on Piz Daint). Compilation options: Build the default option (uses use_torc=0): make Build the OpenMP version: make use_omp = 1 Build the TORC-based version: make use_torc = 1 Build the serial version: make use_omp = 0 use_torc = 0 Test coming soon Notes Please send your questions to: garampat AT ethz.ch","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#prerequisities","text":"An MPI implementation must be installed on your system, preferably with full thread safety, e.g., MPICH . GSL-2.4 or later must be installed on your system.","title":"Prerequisities"},{"location":"installation/#installation-steps","text":"","title":"Installation steps"},{"location":"installation/#0-download","text":"Download the Korali project from GitHub .","title":"0. Download"},{"location":"installation/#1-torc-library","text":"The Torc library is shipped with the Korali code. Torc is tasking library that enables platform-independent code. In this example we assume that the MPI compiler is named mpicc: cd lib/torc_lite autoreconf ./configure CC = mpicc --prefix = $HOME /usr/torc make ; make install export PATH = $HOME /usr/torc/bin: $PATH After installing torc, the following flags are available: torc_cflags torc_libs","title":"1. Torc Library"},{"location":"installation/#2-gsl-library","text":"The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. Download and unpack the latest version of GSL (outside of korali). Execute following commands in your terminal: wget http://mirror.easyname.at/gnu/gsl/gsl-latest.tar.gz tar -xvzf gsl-latest.tar.gz Configure GSL. Set the install folder to be /$HOME/usr . If you want to install in the default diretcory, /usr/local , delete the --prefix=/$HOME/usr . ./configure --prefix = / $HOME /usr Compile and install GSL. make -j2 make install This step will take some time. If your machine has more than two cores, set the 2 in the -j2 to that number number.","title":"2. GSL Library"},{"location":"installation/#gsl-installation-hints","text":"Read the INSTALL notes of GSL. Update the path variable of your terminal with the location of the binaries (gsl-config, gsl-histogram and gsl-randist). Set the LD_LIBRARY_PATH variable of your terminal to the newly installed lib folder.","title":"GSL Installation Hints"},{"location":"installation/#3-korali-installation","text":"Enter the build directory: cd build Before compiling, the following need to be checked: Name for the MPI compiler in the source/make/common.mk , since this can be named differently on different platforms (e.g. CC=cc on Piz Daint). Compilation options: Build the default option (uses use_torc=0): make Build the OpenMP version: make use_omp = 1 Build the TORC-based version: make use_torc = 1 Build the serial version: make use_omp = 0 use_torc = 0","title":"3. Korali Installation"},{"location":"installation/#test","text":"coming soon","title":"Test"},{"location":"installation/#notes","text":"Please send your questions to: garampat AT ethz.ch","title":"Notes"},{"location":"references/","text":"Related publications Pi4U framework Hadjidoukas P.E., Angelikopoulos P., Papadimitriou C., Koumoutsakos P., Pi4U: A high performance computing framework for Bayesian uncertainty quantification of complex models. J. Comput. Phys., 284:1-21, 2015 ( doi , pdf ) Hadjidoukas P.E., Angelikopoulos P., Kulakova L., Papadimitriou C., Koumoutsakos P., Exploiting Task-Based Parallelism in Bayesian Uncertainty Quantification. EuroPar 2015, LLCS 2015, 9233, 532 ( doi , pdf ) Applications Kulakova L., Angelikopoulos P., Hadjidoukas P. E., Papadimitriou C., Koumoutsakos P., Approximate Bayesian Computation for Granular and Molecular Dynamics Simulations. Proceedings of the Platform for Advanced Scientific Computing Conference PASC'16, 2016 ( doi , pdf ) Hadjidoukas P.E, Angelikopoulos P., Rossinelli D., Alexeev D., Papadimitriou C., Koumoutsakos P., Bayesian uncertainty quantification and propagation for discrete element simulations of granular materials. Comput. Methods Appl. Mech. Engrg., 282:218-238, 2014 ( doi , pdf ) TORC: Task-Based Runtime Library Hadjidoukas P.E., Lappas E., Dimakopoulos V.V: A Runtime Library for Platform-Independent Task Parallelism. PDP 2012: 229-236 ( doi )","title":"References"},{"location":"references/#related-publications","text":"","title":"Related publications"},{"location":"references/#pi4u-framework","text":"Hadjidoukas P.E., Angelikopoulos P., Papadimitriou C., Koumoutsakos P., Pi4U: A high performance computing framework for Bayesian uncertainty quantification of complex models. J. Comput. Phys., 284:1-21, 2015 ( doi , pdf ) Hadjidoukas P.E., Angelikopoulos P., Kulakova L., Papadimitriou C., Koumoutsakos P., Exploiting Task-Based Parallelism in Bayesian Uncertainty Quantification. EuroPar 2015, LLCS 2015, 9233, 532 ( doi , pdf )","title":"Pi4U framework"},{"location":"references/#applications","text":"Kulakova L., Angelikopoulos P., Hadjidoukas P. E., Papadimitriou C., Koumoutsakos P., Approximate Bayesian Computation for Granular and Molecular Dynamics Simulations. Proceedings of the Platform for Advanced Scientific Computing Conference PASC'16, 2016 ( doi , pdf ) Hadjidoukas P.E, Angelikopoulos P., Rossinelli D., Alexeev D., Papadimitriou C., Koumoutsakos P., Bayesian uncertainty quantification and propagation for discrete element simulations of granular materials. Comput. Methods Appl. Mech. Engrg., 282:218-238, 2014 ( doi , pdf )","title":"Applications"},{"location":"developing/beta/","text":"coming soon","title":"Beta"},{"location":"developing/conventions/","text":"coming soon","title":"Conventions"},{"location":"developing/developing/","text":"Repository Structure TODO Conventions TODO Write your own likelihood TODO Upcoming ABC-SubSim (xx/xx/2018) AMaLGaM (xx/xx/2018) Multi-objective optimization (xx/xx/2018) Manifold TMCMC (xx/xx/2018) Beta python interface matlab interface","title":"Repository Structure"},{"location":"developing/developing/#repository-structure","text":"TODO","title":"Repository Structure"},{"location":"developing/developing/#conventions","text":"TODO","title":"Conventions"},{"location":"developing/developing/#write-your-own-likelihood","text":"TODO","title":"Write your own likelihood"},{"location":"developing/developing/#upcoming","text":"ABC-SubSim (xx/xx/2018) AMaLGaM (xx/xx/2018) Multi-objective optimization (xx/xx/2018) Manifold TMCMC (xx/xx/2018)","title":"Upcoming"},{"location":"developing/developing/#beta","text":"python interface matlab interface","title":"Beta"},{"location":"developing/likelihoods/","text":"coming soon","title":"Likelihoods"},{"location":"developing/par_files/","text":"priors.par tmcmc.par dram.par dram parameter file generator","title":"Par files"},{"location":"developing/par_files/#priorspar","text":"","title":"priors.par"},{"location":"developing/par_files/#tmcmcpar","text":"","title":"tmcmc.par"},{"location":"developing/par_files/#drampar","text":"dram parameter file generator","title":"dram.par"},{"location":"developing/python/","text":"Python interface of TMCMC Module tmcmc implements the TMCMC (Transitional Markov-Chain Monte Carlo) algorithm. We use the C implementation of the algorithm and the ctypes wrapper module for calling it from Python. Since the function we are sampling from is also written in Python, we use embedded Python to call the function from the C code and numpy to pass arguments between the two environments. TMCMC The Python interface of TMCMC is defined as follows: def tmcmc(fitfun, dim=2, maxstages=20, popsize=1024, lowerbound=[-6,-6], upperbound=[6,6], id=0): Inputs: fitfun : python script name (without the .py extension) that implements the log-likelihood function. The prototype is: fitfun(x, dim), where x is a vector of parameter values and dim the number of parameters. dim : number of parameters maxstages : maximum number of stages that can be performed by the TMCMC algorithm popsize : number of samples per stage lowerbound : lower bound for each parameter upperbound : uppoer bound for each parameter id : integer identifier for the specific tmcmc() call. The value is appended to the output files generated by the algorithm. Outputs: The tmcmc function returns the log-evidence of the model. In addition, for each stage of the algorithm, the evaluation points and their log-likelihood are stored in text files named as curgen_db_xxx_yyy.txt , where xxx : id of the tmcmc() call, and yyy : stage of the algorithm. Fitness function The fitness function typically computes the model output and returns the log-likelihood of the data. def fitfun(theta, dim): theta : parameters of the model dim : number of parameters Plotting 2D scatter plots of the samples stored in the output text files, colored according to their log-likelihood values, can be produced by means of the plot_gen() function. def plot_gen(filename, dim=2, i=1, j=2, show=1, save=0): filename : curgen_db_xxx_yyy.txt file dim : number of parameters i : i-th parameter j : j-th parameter namei : label for the i-th parameter namej : label for the j-th parameter show : depict the plot on the screen save : save the plot in a png file (filename.png) Example demo3.py # Import necessary modules import sys sys . path . append ( ../lib_python ) from tmcmc import * from plot_gen import * logEv = tmcmc ( fitfun3 , dim = 2 , maxstages = 20 , popsize = 4096 , lowerbound = [ - 10 , - 10 ], upperbound = [ 10 , 10 ], id = 3 ) print ( logEv= , logEv [ 0 ]) plot_gen ( curgen_db_003_009.txt , dim = 2 , i = 1 , j = 2 , namei = alpha , namej = sigma^2 , show = 1 , save = 0 ) fitfun3.py # data-driven inference # underlying data model: y = ax + e, where # a=0.3 and e=N(0,sigma^2), sigma=0.1 import numpy as np import math def fitfun3 ( theta , dim ): a = theta [ 0 ] sigma = theta [ 1 ] data = np . loadtxt ( data3.txt ) N = data . shape [ 0 ]; x = data [:, 0 ] dy = data [:, 1 ] y = a * x ; SSE = np . sum (( y - dy ) \\ * \\ * 2 ) sy = sigma ; logn = - 0.5 * N * math . log ( 2 * math . pi ) - 0.5 * N * math . log ( sy * sy ) - 0.5 * SSE / ( sy * sy ) return logn Installation and testing Prerequisites Make sure that the following required software has been installed. GCC compiler Python 2.7 (or higher) Python-devel package python2-config or python3-config must be available MacOS: automatically available if python has been installed with brew Python numpy package MacOS: pip2 install numpy or pip3 install numpy Python matplotlib package MacOS: pip2 install matplotlib or pip3 install matplotlib GNU GSL library MacOS: brew install gsl We also provide the option to download and use a local copy of the GSL library Installation steps 1. Download from GitHub: git clone https://github.com/cselab/pi4u.git pi4u-tmcmc -b tmcmc If you do not have git, download pi4u-tmcmc.zip from here: GitHub download link 2. Go to pi4u-tmcmc/src and build the TMCMC library: cd pi4u-tmcmc/src; make cd .. ( go back to the pi4u-tmcmc directory ) Testing - Running the demo 1. Go to pi4u-tmcmc/demo_python and run demo3.py cd demo_python; python2 demo3.py 2. The scatter plot of samples for our example should be as follows: Troubleshooting and additional options How to build the software using a local copy of the GSL library 1. Go to pi4u-tmcmc/gsl and run build.sh: cd pi4u-tmcmc/gsl; ./build.sh cd .. ( go back to the pi4u-tmcmc directory ) 2. Go to pi4u-tmcmc/src and build the TMCMC library: cd src; make -B mygsl=1 How to build the software using python3 1. Go to pi4u-tmcmc/src and (re)-build the TMCMC library: make -B python3=1 cd .. ( go back to the pi4u-tmcmc directory ) 2. Go to demo_python and run demo3.py using python3 cd demo_python; python3 demo3.py","title":"Python interface of TMCMC"},{"location":"developing/python/#python-interface-of-tmcmc","text":"Module tmcmc implements the TMCMC (Transitional Markov-Chain Monte Carlo) algorithm. We use the C implementation of the algorithm and the ctypes wrapper module for calling it from Python. Since the function we are sampling from is also written in Python, we use embedded Python to call the function from the C code and numpy to pass arguments between the two environments.","title":"Python interface of TMCMC"},{"location":"developing/python/#tmcmc","text":"The Python interface of TMCMC is defined as follows: def tmcmc(fitfun, dim=2, maxstages=20, popsize=1024, lowerbound=[-6,-6], upperbound=[6,6], id=0): Inputs: fitfun : python script name (without the .py extension) that implements the log-likelihood function. The prototype is: fitfun(x, dim), where x is a vector of parameter values and dim the number of parameters. dim : number of parameters maxstages : maximum number of stages that can be performed by the TMCMC algorithm popsize : number of samples per stage lowerbound : lower bound for each parameter upperbound : uppoer bound for each parameter id : integer identifier for the specific tmcmc() call. The value is appended to the output files generated by the algorithm. Outputs: The tmcmc function returns the log-evidence of the model. In addition, for each stage of the algorithm, the evaluation points and their log-likelihood are stored in text files named as curgen_db_xxx_yyy.txt , where xxx : id of the tmcmc() call, and yyy : stage of the algorithm.","title":"TMCMC"},{"location":"developing/python/#fitness-function","text":"The fitness function typically computes the model output and returns the log-likelihood of the data. def fitfun(theta, dim): theta : parameters of the model dim : number of parameters","title":"Fitness function"},{"location":"developing/python/#plotting","text":"2D scatter plots of the samples stored in the output text files, colored according to their log-likelihood values, can be produced by means of the plot_gen() function. def plot_gen(filename, dim=2, i=1, j=2, show=1, save=0): filename : curgen_db_xxx_yyy.txt file dim : number of parameters i : i-th parameter j : j-th parameter namei : label for the i-th parameter namej : label for the j-th parameter show : depict the plot on the screen save : save the plot in a png file (filename.png)","title":"Plotting"},{"location":"developing/python/#example","text":"","title":"Example"},{"location":"developing/python/#demo3py","text":"# Import necessary modules import sys sys . path . append ( ../lib_python ) from tmcmc import * from plot_gen import * logEv = tmcmc ( fitfun3 , dim = 2 , maxstages = 20 , popsize = 4096 , lowerbound = [ - 10 , - 10 ], upperbound = [ 10 , 10 ], id = 3 ) print ( logEv= , logEv [ 0 ]) plot_gen ( curgen_db_003_009.txt , dim = 2 , i = 1 , j = 2 , namei = alpha , namej = sigma^2 , show = 1 , save = 0 )","title":"demo3.py"},{"location":"developing/python/#fitfun3py","text":"# data-driven inference # underlying data model: y = ax + e, where # a=0.3 and e=N(0,sigma^2), sigma=0.1 import numpy as np import math def fitfun3 ( theta , dim ): a = theta [ 0 ] sigma = theta [ 1 ] data = np . loadtxt ( data3.txt ) N = data . shape [ 0 ]; x = data [:, 0 ] dy = data [:, 1 ] y = a * x ; SSE = np . sum (( y - dy ) \\ * \\ * 2 ) sy = sigma ; logn = - 0.5 * N * math . log ( 2 * math . pi ) - 0.5 * N * math . log ( sy * sy ) - 0.5 * SSE / ( sy * sy ) return logn","title":"fitfun3.py"},{"location":"developing/python/#installation-and-testing","text":"","title":"Installation and testing"},{"location":"developing/python/#prerequisites","text":"Make sure that the following required software has been installed. GCC compiler Python 2.7 (or higher) Python-devel package python2-config or python3-config must be available MacOS: automatically available if python has been installed with brew Python numpy package MacOS: pip2 install numpy or pip3 install numpy Python matplotlib package MacOS: pip2 install matplotlib or pip3 install matplotlib GNU GSL library MacOS: brew install gsl We also provide the option to download and use a local copy of the GSL library","title":"Prerequisites"},{"location":"developing/python/#installation-steps","text":"1. Download from GitHub: git clone https://github.com/cselab/pi4u.git pi4u-tmcmc -b tmcmc If you do not have git, download pi4u-tmcmc.zip from here: GitHub download link 2. Go to pi4u-tmcmc/src and build the TMCMC library: cd pi4u-tmcmc/src; make cd .. ( go back to the pi4u-tmcmc directory )","title":"Installation steps"},{"location":"developing/python/#testing-running-the-demo","text":"1. Go to pi4u-tmcmc/demo_python and run demo3.py cd demo_python; python2 demo3.py 2. The scatter plot of samples for our example should be as follows:","title":"Testing - Running the demo"},{"location":"developing/python/#troubleshooting-and-additional-options","text":"","title":"Troubleshooting and additional options"},{"location":"developing/python/#how-to-build-the-software-using-a-local-copy-of-the-gsl-library","text":"1. Go to pi4u-tmcmc/gsl and run build.sh: cd pi4u-tmcmc/gsl; ./build.sh cd .. ( go back to the pi4u-tmcmc directory ) 2. Go to pi4u-tmcmc/src and build the TMCMC library: cd src; make -B mygsl=1","title":"How to build the software using a local copy of the GSL library"},{"location":"developing/python/#how-to-build-the-software-using-python3","text":"1. Go to pi4u-tmcmc/src and (re)-build the TMCMC library: make -B python3=1 cd .. ( go back to the pi4u-tmcmc directory ) 2. Go to demo_python and run demo3.py using python3 cd demo_python; python3 demo3.py","title":"How to build the software using python3"},{"location":"developing/structure/","text":"coming soon","title":"Structure"},{"location":"developing/upcoming/","text":"ABC-SubSim (xx/xx/2018) AMaLGaM (xx/xx/2018) Multi-objective optimization (xx/xx/2018) Manifold TMCMC (xx/xx/2018)","title":"Upcoming"},{"location":"examples/external/","text":"Coupling with external code In this example we sample the posterior distribution of the parameters of an externally defined model and likelihood function, i.e. the TMCMC library calls a shell script that executes two python scripts for the evaluation of the model and the loglikelihood. Here we build the code with the tasking library TORC for clusters. To build the example: cd build make tmcmc_theta_external use_torc = 1 To setup the example: cd ../examples/sampling/external/tmcmc ./setup_tmcmc.sh cd runs/run_001 Contents of runs/run_001 : The korali executable: tmcmc_theta_external The files priors.par and tmcmc.par . These files contain information about the choice of the prior distributions and the TMCMC parameters respectively. A directory named model . Inside this directory, korali expects to find: a file with the experimental data named data.txt a user-provided script named doall.sh , that is called from within the TMCMC library and which (i) runs the external simulation, (ii) compares the output with the experimental data, and (iii) saves the log-likelihood inside a file called loglike.txt . The value inside loglike.txt is then read from korali. To run the example: This example creates intermediate folders of the form tmpdir.*.*.*.* whilst running on a set of parameters saved in params.txt . After the execution the directory is deleted. If you want to keep the directories set REMOVEDIRS 0 in source/likelihoods/loglike_theta.c . mpirun -np 4 ./tmcmc_theta_external or export TORC_WORKERS = 4 mpirun -np 1 ./tmcmc_theta_external Note that the evaluation of externally defined models slow down the execution of the code. You might want to reduce the population size in tmcmc.par in order to quickly test this example. We recommend to use externally defined models for model evaluations that have a longer execution time such that the runtime is not dominated by switching inbetween processes.","title":"External"},{"location":"examples/external/#coupling-with-external-code","text":"In this example we sample the posterior distribution of the parameters of an externally defined model and likelihood function, i.e. the TMCMC library calls a shell script that executes two python scripts for the evaluation of the model and the loglikelihood. Here we build the code with the tasking library TORC for clusters. To build the example: cd build make tmcmc_theta_external use_torc = 1 To setup the example: cd ../examples/sampling/external/tmcmc ./setup_tmcmc.sh cd runs/run_001 Contents of runs/run_001 : The korali executable: tmcmc_theta_external The files priors.par and tmcmc.par . These files contain information about the choice of the prior distributions and the TMCMC parameters respectively. A directory named model . Inside this directory, korali expects to find: a file with the experimental data named data.txt a user-provided script named doall.sh , that is called from within the TMCMC library and which (i) runs the external simulation, (ii) compares the output with the experimental data, and (iii) saves the log-likelihood inside a file called loglike.txt . The value inside loglike.txt is then read from korali. To run the example: This example creates intermediate folders of the form tmpdir.*.*.*.* whilst running on a set of parameters saved in params.txt . After the execution the directory is deleted. If you want to keep the directories set REMOVEDIRS 0 in source/likelihoods/loglike_theta.c . mpirun -np 4 ./tmcmc_theta_external or export TORC_WORKERS = 4 mpirun -np 1 ./tmcmc_theta_external Note that the evaluation of externally defined models slow down the execution of the code. You might want to reduce the population size in tmcmc.par in order to quickly test this example. We recommend to use externally defined models for model evaluations that have a longer execution time such that the runtime is not dominated by switching inbetween processes.","title":"Coupling with external code"},{"location":"examples/hierarchical/","text":"Hierarchical Bayesian Model Synthetic Data In this example we show how to sample the posterior distribution in an uncertainty quantification problem. First, we create 5 sets of synthetic data using the model below, f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ). f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ). For each set we fix the parameter \\varphi_i^{\\star} \\varphi_i^{\\star} and create 20 20 data points using the equation, d_{i,j} = f(x_j,\\varphi_i^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , d_{i,j} = f(x_j,\\varphi_i^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , where x_j = 0.1 j,\\; j=1,\\ldots,20 x_j = 0.1 j,\\; j=1,\\ldots,20 and \\sigma=0.3 \\sigma=0.3 . The index i i indicates the 5 different sets. The parameters for each set were chosen randomly according to \\varphi_i^{\\star} = (2,3,1) + 0.4\\zeta, \\quad \\zeta \\sim \\mathcal{N}(0,1). \\varphi_i^{\\star} = (2,3,1) + 0.4\\zeta, \\quad \\zeta \\sim \\mathcal{N}(0,1). These are the 5 data sets: We want to sample the posterior distribution of each \\vartheta_i=(\\varphi_i,\\sigma) \\vartheta_i=(\\varphi_i,\\sigma) conditioned on all the data d=\\{d_1,\\ldots,d_5\\} d=\\{d_1,\\ldots,d_5\\} . The prior distribution is uniform for each parameter, \\begin{align} p(\\vartheta_1) = \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) = \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) = \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) = \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} \\begin{align} p(\\vartheta_1) &= \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) &= \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) &= \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) &= \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} Notes on HB Build executables In the build folder /korali/build run following commands make tmcmc_theta_internal make tmcmc_psi make tmcmc_posterior_theta_internal in order to execute the steps within Phase 1 - 3. Phase 1: sample from the instrumental distribution In the build folder execute following commands in order to change directory and run phase 1: cd ../examples/hierarchical/internal/ ./setup_phase_1.sh ./run_phase_1.sh And then visualize the results analogous to the other examples. Phase 2: sample hyper-parameters In directory korali/examples/hierarchical/internal/ run the commands ./setup_phase_2.sh ./run_phase_2.sh and then visualize the results. Phase 3: sample posterior parameters Run the commands ./setup_phase_3.sh ./run_phase_3.sh and then visualize the results.","title":"Hierarchical"},{"location":"examples/hierarchical/#hierarchical-bayesian-model","text":"","title":"Hierarchical Bayesian Model"},{"location":"examples/hierarchical/#synthetic-data","text":"In this example we show how to sample the posterior distribution in an uncertainty quantification problem. First, we create 5 sets of synthetic data using the model below, f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ). f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ). For each set we fix the parameter \\varphi_i^{\\star} \\varphi_i^{\\star} and create 20 20 data points using the equation, d_{i,j} = f(x_j,\\varphi_i^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , d_{i,j} = f(x_j,\\varphi_i^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , where x_j = 0.1 j,\\; j=1,\\ldots,20 x_j = 0.1 j,\\; j=1,\\ldots,20 and \\sigma=0.3 \\sigma=0.3 . The index i i indicates the 5 different sets. The parameters for each set were chosen randomly according to \\varphi_i^{\\star} = (2,3,1) + 0.4\\zeta, \\quad \\zeta \\sim \\mathcal{N}(0,1). \\varphi_i^{\\star} = (2,3,1) + 0.4\\zeta, \\quad \\zeta \\sim \\mathcal{N}(0,1). These are the 5 data sets: We want to sample the posterior distribution of each \\vartheta_i=(\\varphi_i,\\sigma) \\vartheta_i=(\\varphi_i,\\sigma) conditioned on all the data d=\\{d_1,\\ldots,d_5\\} d=\\{d_1,\\ldots,d_5\\} . The prior distribution is uniform for each parameter, \\begin{align} p(\\vartheta_1) = \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) = \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) = \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) = \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} \\begin{align} p(\\vartheta_1) &= \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) &= \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) &= \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) &= \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} Notes on HB","title":"Synthetic Data"},{"location":"examples/hierarchical/#build-executables","text":"In the build folder /korali/build run following commands make tmcmc_theta_internal make tmcmc_psi make tmcmc_posterior_theta_internal in order to execute the steps within Phase 1 - 3.","title":"Build executables"},{"location":"examples/hierarchical/#phase-1-sample-from-the-instrumental-distribution","text":"In the build folder execute following commands in order to change directory and run phase 1: cd ../examples/hierarchical/internal/ ./setup_phase_1.sh ./run_phase_1.sh And then visualize the results analogous to the other examples.","title":"Phase 1: sample from the instrumental distribution"},{"location":"examples/hierarchical/#phase-2-sample-hyper-parameters","text":"In directory korali/examples/hierarchical/internal/ run the commands ./setup_phase_2.sh ./run_phase_2.sh and then visualize the results.","title":"Phase 2: sample hyper-parameters"},{"location":"examples/hierarchical/#phase-3-sample-posterior-parameters","text":"Run the commands ./setup_phase_3.sh ./run_phase_3.sh and then visualize the results.","title":"Phase 3: sample posterior parameters"},{"location":"examples/optimizing/","text":"Optimizing a posterior distribution In this example we will show how to compute the maximum a posteriori (MAP) estimate of the posterior distribution in an uncertainty quantification problem. First, we will create synthetic data using the model, f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) We fix \\varphi^{\\star} = (2,3,1) \\varphi^{\\star} = (2,3,1) and create 100 100 data point using the equation, d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , where x_i = 0.02 i,\\; i=1,\\ldots,100 x_i = 0.02 i,\\; i=1,\\ldots,100 and \\sigma=0.3 \\sigma=0.3 . We want to optimize the the posterior distribution of \\vartheta=(\\varphi,\\sigma) \\vartheta=(\\varphi,\\sigma) conditioned on the data d d . The prior distribution is uniform for each parameter, \\begin{align} p(\\vartheta_1) = \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) = \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) = \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) = \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} \\begin{align} p(\\vartheta_1) &= \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) &= \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) &= \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) &= \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} and the likelihood function in given by, \\begin{align} p(d | \\vartheta) = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ = \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} \\begin{align} p(d | \\vartheta) & = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ &= \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} Finally, we want to compute, $$ \\vartheta^\\star = \\mathop{\\arg\\max}\\limits_{\\vartheta} \\,\\, p( \\vartheta | d) \\,. $$ Optimizing with CMA-ES Here we use the Covariance Matrix Adaptation - Evolution Strategy (CMA-ES) algorithm in order to optimize the posterior distribution p(d | \\vartheta) p(d | \\vartheta) . Compile and run From the base folder run cd build make cmaes_theta_internal Make sure that we set use_torc=0 and use_omp=0 in the Makefile since we don't want to run the code in parallel. In the build folder execute following commands in order to setup the example: cd ../examples/optimization/internal/ ./setup_fast_optimize.sh cd runs/run_001/ Run the CMA-ES optimization algorithm: ./cmaes_theta_internal The output in the terminal will look like this: The algorithm has converged to 2.0392035199474865 2.9457365262310957 1.0625543252360856 0.3022538646514062 Note that the values \\vartheta^{\\star} = (2,3,1,0.3) \\vartheta^{\\star} = (2,3,1,0.3) have been used to create the synthetic data. Finally, visualize the samples: cp ../../../../../source/tools/postprocessing_tools/cmaes/cmaplt.py . python plotmatrix_hist.py final.txt Behind the scripts The script ./setup_tmcmc.sh creates new folders and copies the executable and the required files to target directory: the data file data.txt containing d_i d_i , the file priors.par that contains the prior information p(\\vartheta_i) p(\\vartheta_i) (i.e number of prior distributions, distribution functions, function parameters), the parameter file for cmaes, cmaes_bounds.par , cmaes_initials.par , cmaes_signals.par . In this example, Korali has been linked to the file loglike_theta_fast.c . Inside this file you can find the model f f as well as the likelihood function p(d | \\vartheta) p(d | \\vartheta) . More information on the implementation of a likelihood can be found here .","title":"Optimizing"},{"location":"examples/optimizing/#optimizing-a-posterior-distribution","text":"In this example we will show how to compute the maximum a posteriori (MAP) estimate of the posterior distribution in an uncertainty quantification problem. First, we will create synthetic data using the model, f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) We fix \\varphi^{\\star} = (2,3,1) \\varphi^{\\star} = (2,3,1) and create 100 100 data point using the equation, d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , where x_i = 0.02 i,\\; i=1,\\ldots,100 x_i = 0.02 i,\\; i=1,\\ldots,100 and \\sigma=0.3 \\sigma=0.3 . We want to optimize the the posterior distribution of \\vartheta=(\\varphi,\\sigma) \\vartheta=(\\varphi,\\sigma) conditioned on the data d d . The prior distribution is uniform for each parameter, \\begin{align} p(\\vartheta_1) = \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) = \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) = \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) = \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} \\begin{align} p(\\vartheta_1) &= \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) &= \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) &= \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) &= \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} and the likelihood function in given by, \\begin{align} p(d | \\vartheta) = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ = \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} \\begin{align} p(d | \\vartheta) & = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ &= \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} Finally, we want to compute, $$ \\vartheta^\\star = \\mathop{\\arg\\max}\\limits_{\\vartheta} \\,\\, p( \\vartheta | d) \\,. $$","title":"Optimizing a posterior distribution"},{"location":"examples/optimizing/#optimizing-with-cma-es","text":"Here we use the Covariance Matrix Adaptation - Evolution Strategy (CMA-ES) algorithm in order to optimize the posterior distribution p(d | \\vartheta) p(d | \\vartheta) .","title":"Optimizing with CMA-ES"},{"location":"examples/optimizing/#compile-and-run","text":"From the base folder run cd build make cmaes_theta_internal Make sure that we set use_torc=0 and use_omp=0 in the Makefile since we don't want to run the code in parallel. In the build folder execute following commands in order to setup the example: cd ../examples/optimization/internal/ ./setup_fast_optimize.sh cd runs/run_001/ Run the CMA-ES optimization algorithm: ./cmaes_theta_internal The output in the terminal will look like this: The algorithm has converged to 2.0392035199474865 2.9457365262310957 1.0625543252360856 0.3022538646514062 Note that the values \\vartheta^{\\star} = (2,3,1,0.3) \\vartheta^{\\star} = (2,3,1,0.3) have been used to create the synthetic data. Finally, visualize the samples: cp ../../../../../source/tools/postprocessing_tools/cmaes/cmaplt.py . python plotmatrix_hist.py final.txt","title":"Compile and run"},{"location":"examples/optimizing/#behind-the-scripts","text":"The script ./setup_tmcmc.sh creates new folders and copies the executable and the required files to target directory: the data file data.txt containing d_i d_i , the file priors.par that contains the prior information p(\\vartheta_i) p(\\vartheta_i) (i.e number of prior distributions, distribution functions, function parameters), the parameter file for cmaes, cmaes_bounds.par , cmaes_initials.par , cmaes_signals.par . In this example, Korali has been linked to the file loglike_theta_fast.c . Inside this file you can find the model f f as well as the likelihood function p(d | \\vartheta) p(d | \\vartheta) . More information on the implementation of a likelihood can be found here .","title":"Behind the scripts"},{"location":"examples/sampling/","text":"Sampling a posterior distribution In this example we will show how to sample the posterior distribution in an uncertainty quantification problem. First, we will create synthetic data using the model, f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) We fix \\varphi^{\\star} = (2,3,1) \\varphi^{\\star} = (2,3,1) and create 100 100 data point using the equation, d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , where x_i = 0.02 i,\\; i=1,\\ldots,100 x_i = 0.02 i,\\; i=1,\\ldots,100 and \\sigma=0.3 \\sigma=0.3 . We will sample the posterior distribution of \\vartheta=(\\varphi,\\sigma) \\vartheta=(\\varphi,\\sigma) conditioned on the data d d . The prior distribution is uniform for each parameter, \\begin{align} p(\\vartheta_1) = \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) = \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) = \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) = \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} \\begin{align} p(\\vartheta_1) &= \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) &= \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) &= \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) &= \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} and the likelihood function in given by, \\begin{align} p(d | \\vartheta) = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ = \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} \\begin{align} p(d | \\vartheta) & = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ &= \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} Sampling with TMCMC Second, we will use the Transitional Markov Chain Monte Carlo (TMCMC) algorithm for the sampling of the posterior distribution of \\vartheta \\vartheta . Compile and run From the base folder run cd build make tmcmc_theta_internal Make sure that we set use_torc=0 and use_omp=0 in the Makefile since we don't want to run the code in parallel. In the build folder execute following commands in order to setup the example: cd ../examples/sampling/internal/tmcmc/ ./setup_tmcmc.sh cd runs/run_001/ Run the TMCMC sampling algorithm: ./tmcmc_theta_internal Finally, visualize the samples: cp ../../../../../../source/tools/display/plotmatrix_hist.py . python plotmatrix_hist.py final.txt Behind the scripts The script ./setup_tmcmc.sh creates new folders and copies the executable and the required files to target directory: the data file data.txt containing d_i d_i , the file priors.par that contains the prior information p(\\vartheta_i) p(\\vartheta_i) (i.e number of prior distributions, distribution functions, function parameters), the parameter file tmcmc.par for tmcmc (e.g. number of samples, max iterations, seed). In this example, Korali has been linked to the file loglike_theta_fast.c . Inside this file you can find the model f f as well as the likelihood function p(d | \\vartheta) p(d | \\vartheta) . More information on the implementation of a likelihood can be found here . Sampling with DRAM Compile and run From the base folder run cd build make dram_theta_internal Make sure that we set use_torc=0 and use_omp=0 in the Makefile since we don't want to run the code in parallel. In the build folder execute following commands in order to setup the example: cd ../examples/sampling/internal/dram/ ./setup_dram.sh cd runs/run_001/ Run the DRAM sampling algorithm: ./dram_theta_internal Finally, visualize the samples: cp ../../../../../../source/tools/display/plotmatrix_hist.py . tail -n +1000 chain.txt tmp ./plotmatrix_hist.py tmp With the command tail -n +1000 chain.txt we discard the first 1000 1000 samples which we consider as the burn-in period. Behind the scripts Similar to the TMCMC example above. The only difference is that instead of the tmcmc.par we use dram.par .","title":"Sampling"},{"location":"examples/sampling/#sampling-a-posterior-distribution","text":"In this example we will show how to sample the posterior distribution in an uncertainty quantification problem. First, we will create synthetic data using the model, f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) f(x;\\varphi) = \\varphi_1 \\sin(\\varphi_2 x + \\varphi_3 ) We fix \\varphi^{\\star} = (2,3,1) \\varphi^{\\star} = (2,3,1) and create 100 100 data point using the equation, d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , d_i = f(x_i,\\varphi^{\\star}) + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1) \\, , where x_i = 0.02 i,\\; i=1,\\ldots,100 x_i = 0.02 i,\\; i=1,\\ldots,100 and \\sigma=0.3 \\sigma=0.3 . We will sample the posterior distribution of \\vartheta=(\\varphi,\\sigma) \\vartheta=(\\varphi,\\sigma) conditioned on the data d d . The prior distribution is uniform for each parameter, \\begin{align} p(\\vartheta_1) = \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) = \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) = \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) = \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} \\begin{align} p(\\vartheta_1) &= \\mathcal{U}( \\vartheta_1 | 0,5) \\\\ p(\\vartheta_2) &= \\mathcal{U}( \\vartheta_2 | 0,10) \\\\ p(\\vartheta_3) &= \\mathcal{U}( \\vartheta_3 | -3.14,3.14) \\\\ p(\\vartheta_4) &= \\mathcal{U}( \\vartheta_4 | 0,5) \\, , \\end{align} and the likelihood function in given by, \\begin{align} p(d | \\vartheta) = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ = \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align} \\begin{align} p(d | \\vartheta) & = \\prod_{i=1}^{100} p(d_i | \\vartheta) \\\\ &= \\prod_{i=1}^{100} \\mathcal{N}( d_i | f(x;\\varphi),\\sigma ) \\, . \\end{align}","title":"Sampling a posterior distribution"},{"location":"examples/sampling/#sampling-with-tmcmc","text":"Second, we will use the Transitional Markov Chain Monte Carlo (TMCMC) algorithm for the sampling of the posterior distribution of \\vartheta \\vartheta .","title":"Sampling with TMCMC"},{"location":"examples/sampling/#compile-and-run","text":"From the base folder run cd build make tmcmc_theta_internal Make sure that we set use_torc=0 and use_omp=0 in the Makefile since we don't want to run the code in parallel. In the build folder execute following commands in order to setup the example: cd ../examples/sampling/internal/tmcmc/ ./setup_tmcmc.sh cd runs/run_001/ Run the TMCMC sampling algorithm: ./tmcmc_theta_internal Finally, visualize the samples: cp ../../../../../../source/tools/display/plotmatrix_hist.py . python plotmatrix_hist.py final.txt","title":"Compile and run"},{"location":"examples/sampling/#behind-the-scripts","text":"The script ./setup_tmcmc.sh creates new folders and copies the executable and the required files to target directory: the data file data.txt containing d_i d_i , the file priors.par that contains the prior information p(\\vartheta_i) p(\\vartheta_i) (i.e number of prior distributions, distribution functions, function parameters), the parameter file tmcmc.par for tmcmc (e.g. number of samples, max iterations, seed). In this example, Korali has been linked to the file loglike_theta_fast.c . Inside this file you can find the model f f as well as the likelihood function p(d | \\vartheta) p(d | \\vartheta) . More information on the implementation of a likelihood can be found here .","title":"Behind the scripts"},{"location":"examples/sampling/#sampling-with-dram","text":"","title":"Sampling with DRAM"},{"location":"examples/sampling/#compile-and-run_1","text":"From the base folder run cd build make dram_theta_internal Make sure that we set use_torc=0 and use_omp=0 in the Makefile since we don't want to run the code in parallel. In the build folder execute following commands in order to setup the example: cd ../examples/sampling/internal/dram/ ./setup_dram.sh cd runs/run_001/ Run the DRAM sampling algorithm: ./dram_theta_internal Finally, visualize the samples: cp ../../../../../../source/tools/display/plotmatrix_hist.py . tail -n +1000 chain.txt tmp ./plotmatrix_hist.py tmp With the command tail -n +1000 chain.txt we discard the first 1000 1000 samples which we consider as the burn-in period.","title":"Compile and run"},{"location":"examples/sampling/#behind-the-scripts_1","text":"Similar to the TMCMC example above. The only difference is that instead of the tmcmc.par we use dram.par .","title":"Behind the scripts"}]}